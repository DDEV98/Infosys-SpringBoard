{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb751568-f31a-4fc9-91de-0e67e843c47f",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb6b1c2e-7ee6-49dd-9bba-93b257f7984e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import *\n",
    "sent = \"India is a republic nation. We are proud Indians\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "850a2297-f785-44ea-a6cf-a88d43177f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\pchands\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82be0c51-e316-454a-a0a7-cc69ce7aad58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n",
      "India\n",
      "republic\n"
     ]
    }
   ],
   "source": [
    "print(len(sent))   #Prints the number of characters\n",
    "print(sent[0:5])   #Prints 'India'\n",
    "print(sent[11:19]) #Prints 'republic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee68e386-49f7-4aed-bf4d-d00f0d688caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['India', 'is', 'a', 'republic', 'nation', '.', 'We', 'are', 'proud', 'Indians']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "264f1f73-9b2a-46df-981f-d67b3e8a082d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', 'India', 'Indians', 'We', 'a', 'are', 'is', 'nation', 'proud', 'republic']\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(sent)\n",
    "vocab = sorted(set(tokens))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebf28482-54f6-4099-96c0-b901ef608006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['India', 'Indians', 'We', 'a', 'are', 'is', 'nation', 'proud', 'republic']\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "\n",
    "vocab_wo_punct = []\n",
    "\n",
    "for i in vocab:\n",
    "    if i not in punctuation:\n",
    "        vocab_wo_punct.append(i)\n",
    "\n",
    "print(vocab_wo_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573b5e87-85ee-4bf0-9240-e4a4696ec6ba",
   "metadata": {},
   "source": [
    "Root of word - Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f141e74-bc64-4f6a-a63e-e169beee9587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['india', 'indian', 'we', 'a', 'are', 'is', 'nation', 'proud', 'republ']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemObj = SnowballStemmer(\"english\")\n",
    "stemObj.stem(\"Studying\")\n",
    "stemmed_vocab=[]\n",
    "stemObj = SnowballStemmer(\"english\")\n",
    "for i in vocab_wo_punct:\n",
    "    stemmed_vocab.append(stemObj.stem(i))\n",
    "print(stemmed_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae20c4ac-aab3-45b1-abbd-512386e78d7a",
   "metadata": {},
   "source": [
    "Base of a word - Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4039f86c-6f0f-4852-aa80-37bbd79621fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmaObj =  WordNetLemmatizer()\n",
    "lemmaObj.lemmatize(\"went\",pos='v')  #Prints 'go'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9414e1e-a48c-458d-8f8b-e6434579af09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['studi', 'studi', 'run', 'ran', 'beauti', 'cat']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# Assume vocab_no_punct is a list of words without punctuation\n",
    "# For demonstration, let's define it:\n",
    "vocab_no_punct = [\"studying\", \"studies\", \"running\", \"ran\", \"beautifully\", \"cats\"]\n",
    "\n",
    "stemobj = SnowballStemmer(\"english\")\n",
    "stemmed_vocab = []\n",
    "\n",
    "for word in vocab_no_punct:\n",
    "    stemmed_vocab.append(stemobj.stem(word))\n",
    "\n",
    "print(stemmed_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77d410f2-6adb-4815-afd2-183b3dca1821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi i'm Doman lal\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "file = open(\"temp.txt\", 'r')\n",
    "text = ''\n",
    "for i in file.readlines():\n",
    "    text += i\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d88ae91d-517b-41f3-9256-fe96919cd3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi i'm Doman lal\n"
     ]
    }
   ],
   "source": [
    "trimmed_text = text.strip()\n",
    "print(trimmed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "728c5379-7739-4598-935c-bd1724053ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi i'm doman lal\n"
     ]
    }
   ],
   "source": [
    "converted_text = trimmed_text.lower()\n",
    "print(converted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0687df61-3ad7-4e8e-9276-ba4dae1b6c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'i', \"'m\", 'doman', 'lal']\n"
     ]
    }
   ],
   "source": [
    "tokenized_list = word_tokenize(converted_text)\n",
    "print(tokenized_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58f0cfac-0a74-4a98-9684-1bd5741b4037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'i', \"'m\", 'doman', 'lal']\n"
     ]
    }
   ],
   "source": [
    "punct_tokenize_list = word_tokenize(converted_text)\n",
    "print(punct_tokenize_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04db52a0-ab96-4deb-9664-b3efb4e0149d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hi', 'i', 'lal', \"'m\", 'doman'}\n"
     ]
    }
   ],
   "source": [
    "vocab_set = set(tokenized_list)\n",
    "print(vocab_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3f6d0f3-ee8b-4cb6-985e-7455c8c0ea53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hi', 'lal', 'doman', \"'m\"}\n"
     ]
    }
   ],
   "source": [
    "set_wo_stopwords = vocab_set - set(stopwords.words(\"english\"))\n",
    "print(set_wo_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffd50b9a-205a-4170-8fc8-46a302d4886c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hi', 'lal', 'doman', \"'m\"}\n"
     ]
    }
   ],
   "source": [
    "set_wo_punctuation = set_wo_stopwords - set(punctuation)\n",
    "print(set_wo_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "267bbab0-35c0-4c9a-9690-795c339e8c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi']\n",
      "['hi', 'lal']\n",
      "['hi', 'lal', 'doman']\n",
      "['hi', 'lal', 'doman', \"'m\"]\n"
     ]
    }
   ],
   "source": [
    "stemmed_list= []\n",
    "stemobj = SnowballStemmer(\"english\")\n",
    "for i in set_wo_punctuation:\n",
    "    stemmed_list.append(stemobj.stem(i))\n",
    "    print(stemmed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "597421c6-9fb3-43cb-bd4d-8c5156ffa607",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\pchands/nltk_data'\n    - \"E:\\\\MCA PU\\\\Internship's\\\\Infosys SpringBoard\\\\my_venv\\\\nltk_data\"\n    - \"E:\\\\MCA PU\\\\Internship's\\\\Infosys SpringBoard\\\\my_venv\\\\share\\\\nltk_data\"\n    - \"E:\\\\MCA PU\\\\Internship's\\\\Infosys SpringBoard\\\\my_venv\\\\lib\\\\nltk_data\"\n    - 'C:\\\\Users\\\\pchands\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#parts of speech tagging\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m pos_tag_list = \u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mset_wo_punctuation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# print(pos_tag_list)\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m#for getting parts of speech\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparts_of_speech\u001b[39m(pos):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\MCA PU\\Internship's\\Infosys SpringBoard\\my_venv\\Lib\\site-packages\\nltk\\tag\\__init__.py:168\u001b[39m, in \u001b[36mpos_tag\u001b[39m\u001b[34m(tokens, tagset, lang)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpos_tag\u001b[39m(tokens, tagset=\u001b[38;5;28;01mNone\u001b[39;00m, lang=\u001b[33m\"\u001b[39m\u001b[33meng\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    144\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[33;03m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[32m    146\u001b[39m \u001b[33;03m    tag the given list of tokens.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    166\u001b[39m \u001b[33;03m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[32m    167\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m     tagger = \u001b[43m_get_tagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\MCA PU\\Internship's\\Infosys SpringBoard\\my_venv\\Lib\\site-packages\\nltk\\tag\\__init__.py:110\u001b[39m, in \u001b[36m_get_tagger\u001b[39m\u001b[34m(lang)\u001b[39m\n\u001b[32m    108\u001b[39m     tagger = PerceptronTagger(lang=lang)\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     tagger = \u001b[43mPerceptronTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\MCA PU\\Internship's\\Infosys SpringBoard\\my_venv\\Lib\\site-packages\\nltk\\tag\\perceptron.py:183\u001b[39m, in \u001b[36mPerceptronTagger.__init__\u001b[39m\u001b[34m(self, load, lang)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28mself\u001b[39m.classes = \u001b[38;5;28mset\u001b[39m()\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_from_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\MCA PU\\Internship's\\Infosys SpringBoard\\my_venv\\Lib\\site-packages\\nltk\\tag\\perceptron.py:273\u001b[39m, in \u001b[36mPerceptronTagger.load_from_json\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_from_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33meng\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    272\u001b[39m     \u001b[38;5;66;03m# Automatically find path to the tagger if location is not specified.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m     loc = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtaggers/averaged_perceptron_tagger_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(loc + TAGGER_JSONS[lang][\u001b[33m\"\u001b[39m\u001b[33mweights\u001b[39m\u001b[33m\"\u001b[39m]) \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[32m    275\u001b[39m         \u001b[38;5;28mself\u001b[39m.model.weights = json.load(fin)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\MCA PU\\Internship's\\Infosys SpringBoard\\my_venv\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\pchands/nltk_data'\n    - \"E:\\\\MCA PU\\\\Internship's\\\\Infosys SpringBoard\\\\my_venv\\\\nltk_data\"\n    - \"E:\\\\MCA PU\\\\Internship's\\\\Infosys SpringBoard\\\\my_venv\\\\share\\\\nltk_data\"\n    - \"E:\\\\MCA PU\\\\Internship's\\\\Infosys SpringBoard\\\\my_venv\\\\lib\\\\nltk_data\"\n    - 'C:\\\\Users\\\\pchands\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "#parts of speech tagging\n",
    "pos_tag_list = pos_tag(set_wo_punctuation)\n",
    "# print(pos_tag_list)\n",
    "#for getting parts of speech\n",
    "def parts_of_speech(pos):\n",
    "    if pos.startswith(\"N\"):\n",
    "        return wordnet.NOUN\n",
    "    elif pos.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    elif pos.startswith(\"V\"):\n",
    "        return wordnet.VERB\n",
    "    elif pos.startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    elif pos.startswith(\"S\"):\n",
    "        return wordnet.ADJ_SAT\n",
    "    else:\n",
    "        return ''\n",
    "#lemmatization\n",
    "lemma_list = []\n",
    "lemmaObj =  WordNetLemmatizer()\n",
    "for word,pos in pos_tag_list:\n",
    "    get_pos = parts_of_speech(pos)\n",
    "    if get_pos != '':\n",
    "        lemma_list.append(lemmaObj.lemmatize(word, pos = get_pos))\n",
    "    else:\n",
    "        lemma_list.append(word)\n",
    "print(lemma_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa58f5a8-90a7-4ea8-a6e0-5088941dc197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hi', 'lal'), ('lal', 'doman'), ('doman', \"'m\")]\n"
     ]
    }
   ],
   "source": [
    "bigrams = ngrams(set_wo_punctuation, 2)\n",
    "print(list(bigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69720558-d8fd-4fc4-a464-6d795c7ffd5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid non-printable character U+00A0 (4210862815.py, line 18)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mcount_upper = lambda x : list(map(str.isupper,x.split())).count(True)\u001b[39m\n                                                                         ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid non-printable character U+00A0\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "punctuation = list(punctuation)\n",
    "# Creating a new feature called Punctuations. \n",
    "# This feature counts the number of punctuation characters in the sms message \n",
    "data[\"Punctuations\"] = data[\"Text\"].apply(lambda x: len(re.findall(r\"[^\\w+&&^\\s]\",x)))\n",
    "# Creating a new feature called Phonenumbers. \n",
    "# This feature indicates if the sms text contains a phonenumber or not\n",
    "data[\"Phonenumbers\"] = data[\"Text\"].apply(lambda x: len(re.findall(r\"[0-9]{10}\",x)))\n",
    "# Creating a new feature called Links.\n",
    "# This feature indicates if the sms text contains a URL or not \n",
    "is_link = lambda x: 1 if re.search(r\"https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+\",x)!=None else 0\n",
    "data[\"Links\"] = data[\"Text\"].apply(is_link)\n",
    "# Creating a new feature called Uppercase.\n",
    "# This feature indicates how many words in the the sms text are in upper case\n",
    "count_upper = lambda x : list(map(str.isupper,x.split())).count(True) \n",
    "upper_case = lambda y,n : n+1 if y.isupper() else n\n",
    "data[\"Uppercase\"] = data[\"Text\"].apply(count_upper)\n",
    "# Identifying and counting how many unusual words are there in the sms text\n",
    "def find_unusual_words(text):\n",
    "    text_vocab_set = set(w.lower() for w in text if w.isalpha())\n",
    "    english_vocab_set = set(w.lower() for w in nltk.corpus.words.words())\n",
    "    unusual_set = text_vocab_set - english_vocab_set\n",
    "    return len(sorted(unusual_set))\n",
    "data[\"unusualwords\"] = data[\"Text\"].apply(lambda x: find_unusual_words(word_tokenize(x)))\n",
    "# View a few records of the data after creating these features\n",
    "print(data[14:25])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c583f3-7655-4266-929c-d2915d518bda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLTK_KERNEL",
   "language": "python",
   "name": "nltk_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
